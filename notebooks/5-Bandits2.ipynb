{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erichsdb/Deep-Reinforcement-Learning/blob/main/notebooks/5-Bandits2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-JNZ02nFN_"
      },
      "source": [
        "# Bandits - part 2\n",
        "\n",
        "In the exercise, we will investigate in more details the properties of the bandit algorithms implemented last time and investigate more advanced ones (explained in the Sutton and Barto book).\n",
        "\n",
        "**Q:** Start by copying all class definitions of the last exercise (Bandit, Greedy, $\\epsilon$-Greedy, softmax) and re-run the experiments with correct values for the parameters in a single cell. We will ignore exploration scheduling (although we should not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L64H_81DnFOB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "class SoftmaxScheduleAgent:\n",
        "    def __init__(self, bandit, alpha, t, tau_decay, q_init=0):\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "        # create an array filled with q_init\n",
        "        self.Q_t = np.zeros(bandit.nb_actions)\n",
        "        self.Q_t.fill(q_init)\n",
        "        self.t = t\n",
        "        self.tau_decay = tau_decay\n",
        "\n",
        "        self.optimal_action = []\n",
        "\n",
        "    def act(self):\n",
        "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.Q_t[action] = self.Q_t[action] + self.alpha * (reward - self.Q_t[action])\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "        for t in range(nb_steps):\n",
        "          action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "\n",
        "          # softmax selection\n",
        "          prob = []\n",
        "\n",
        "          for a in range(self.bandit.nb_actions):\n",
        "            sum = 0\n",
        "            for b in range(self.bandit.nb_actions):\n",
        "                sum += np.exp(((self.Q_t[b] - self.Q_t.max())/self.t))\n",
        "            prob.append(np.exp((self.Q_t[a] - self.Q_t.max()) / self.t) / sum )\n",
        "\n",
        "          # chose reward based on probabilites\n",
        "          action = rng.choice(range(self.bandit.nb_actions), p=prob)\n",
        "\n",
        "          reward = self.bandit.step(action)\n",
        "          self.update(action, reward)\n",
        "\n",
        "\n",
        "          if a == self.bandit.a_star:\n",
        "            self.optimal_action.append(1)\n",
        "          else:\n",
        "            self.optimal_action.append(0)\n",
        "\n",
        "          if (self.t > self.tau_decay): self.t -= self.tau_decay\n",
        "\n",
        "class SoftmaxAgent:\n",
        "    def __init__(self, bandit, alpha, t, q_init):\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "        # create an array filled with q_init\n",
        "        self.Q_t = np.zeros(bandit.nb_actions)\n",
        "        self.Q_t.fill(q_init)\n",
        "        self.t = t\n",
        "\n",
        "        self.optimal_action = []\n",
        "\n",
        "    def act(self):\n",
        "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.Q_t[action] = self.Q_t[action] + self.alpha * (reward - self.Q_t[action])\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "        for t in range(nb_steps):\n",
        "          action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "\n",
        "          # softmax selection\n",
        "          prob = []\n",
        "\n",
        "          for a in range(self.bandit.nb_actions):\n",
        "            sum = 0\n",
        "            for b in range(self.bandit.nb_actions):\n",
        "                sum += np.exp(((self.Q_t[b] - self.Q_t.max())/self.t))\n",
        "            prob.append(np.exp((self.Q_t[a] - self.Q_t.max()) / self.t) / sum )\n",
        "\n",
        "          # chose reward based on probabilites\n",
        "          action = rng.choice(range(self.bandit.nb_actions), p=prob)\n",
        "\n",
        "          reward = self.bandit.step(action)\n",
        "          self.update(action, reward)\n",
        "\n",
        "\n",
        "          if a == self.bandit.a_star:\n",
        "            self.optimal_action.append(1)\n",
        "          else:\n",
        "            self.optimal_action.append(0)\n",
        "\n",
        "class EpsilonGreedyAgent:\n",
        "    def __init__(self, bandit, alpha, epsilon, q_init=0):\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "        # create an array filled with q_init\n",
        "        self.Q_t = np.zeros(bandit.nb_actions)\n",
        "        self.Q_t.fill(q_init)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.optimal_action = []\n",
        "\n",
        "    def act(self):\n",
        "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.Q_t[action] = self.Q_t[action] + self.alpha * (reward - self.Q_t[action])\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "        for t in range(nb_steps):\n",
        "          action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "\n",
        "          # eta selection\n",
        "          if rng.random() < self.epsilon:\n",
        "            action = rng.choice([i for i in range(self.bandit.nb_actions) if i != action])\n",
        "\n",
        "          reward = self.bandit.step(action)\n",
        "          self.update(action, reward)\n",
        "\n",
        "\n",
        "          if action == self.bandit.a_star:\n",
        "            self.optimal_action.append(1)\n",
        "          else:\n",
        "            self.optimal_action.append(0)\n",
        "\n",
        "class GreedyAgent:\n",
        "    def __init__(self, bandit, alpha, q_init):\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "        self.Q_t = np.zeros(bandit.nb_actions)\n",
        "        self.Q_t.fill(q_init)\n",
        "\n",
        "        self.optimal_action = []\n",
        "\n",
        "    def act(self):\n",
        "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.Q_t[action] = self.Q_t[action] + self.alpha * (reward - self.Q_t[action])\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "        for t in range(nb_steps):\n",
        "          action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "          reward = self.bandit.step(action)\n",
        "          self.update(action, reward)\n",
        "\n",
        "\n",
        "          if action == self.bandit.a_star:\n",
        "            self.optimal_action.append(1)\n",
        "          else:\n",
        "            self.optimal_action.append(0)\n",
        "\n",
        "class Bandit:\n",
        "    \"\"\"\n",
        "    n-armed bandit.\n",
        "    \"\"\"\n",
        "    def __init__(self, nb_actions, mean=0.0, std_Q=1.0, std_r=1.0):\n",
        "        \"\"\"\n",
        "        :param nb_actions: number of arms.\n",
        "        :param mean: mean of the normal distribution for $Q^*$.\n",
        "        :param std_Q: standard deviation of the normal distribution for $Q^*$.\n",
        "        :param std_r: standard deviation of the normal distribution for the sampled rewards.\n",
        "        \"\"\"\n",
        "        # Store parameters\n",
        "        self.nb_actions = nb_actions\n",
        "        self.mean = mean\n",
        "        self.std_Q = std_Q\n",
        "        self.std_r = std_r\n",
        "\n",
        "        # Initialize the true Q-values\n",
        "        self.Q_star = rng.normal(self.mean, self.std_Q, self.nb_actions)\n",
        "\n",
        "        # Optimal action\n",
        "        self.a_star = self.Q_star.argmax()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Sampled a single reward from the bandit.\n",
        "\n",
        "        :param action: the selected action.\n",
        "        :return: a reward.\n",
        "        \"\"\"\n",
        "        return float(rng.normal(self.Q_star[action], self.std_r, 1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAo-KHxpnFOC"
      },
      "source": [
        "## Reward distribution\n",
        "\n",
        "We are now going to vary the reward distributions and investigate whether the previous experimental results still hold when the true Q-values are in $\\mathcal{N}(0, 1)$ and the rewards have a variance of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JtNJPHXnFOG"
      },
      "source": [
        "**Q:** Let's now change the distribution of true Q-values from $\\mathcal{N}(0, 1)$ to $\\mathcal{N}(10, 10)$ when creating the bandits and re-run the algorithms. What happens and why? Modify the values of `epsilon` and `tau` to try to get a better behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8OIPNj5cnFOH",
        "outputId": "342f8a47-0fde-467e-a7df-757b5e7ece81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: [0. 0. 0. 0. 0.]\n",
            "0: 0.245\n",
            "1: [0.09717083 0.00880566 0.09656495 0.10559485 0.11674778]\n",
            "1: 0.865\n",
            "2: [0.24179169 0.31791526 0.12978829 0.27649875 0.34898556]\n",
            "2: 0.895\n",
            "3: [0.54464035 0.29473889 0.27222837 0.59921228 0.31295425]\n",
            "3: 0.85\n",
            "4: [0.80743012 0.22134167 0.47376885 0.61987232 0.32861217]\n",
            "4: 0.835\n",
            "5: [0.40553414 0.31255249 0.59883902 0.93416795 0.39303285]\n",
            "5: 0.825\n",
            "6: [0.33247188 0.83521469 0.79207664 0.56063425 0.83384439]\n",
            "6: 0.78\n",
            "7: [0.5977077  0.53009592 0.62455926 0.94059312 0.38791304]\n",
            "7: 0.815\n",
            "8: [1.07996162 1.09486432 0.68754277 1.42941    0.60519221]\n",
            "8: 0.805\n",
            "9: [0.59506235 0.81976838 1.22108042 0.89868732 1.49713611]\n",
            "9: 0.78\n"
          ]
        }
      ],
      "source": [
        "nb_actions = 5\n",
        "\n",
        "# values from 0 - 10\n",
        "mean_values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "for mean in mean_values:\n",
        "  rewards_epsilon_softmax = []\n",
        "  optimal_epsilon_softmax = []\n",
        "\n",
        "  for i in range(200):\n",
        "    bandit = Bandit(nb_actions, mean=0, std_Q=mean, std_r=mean)\n",
        "    agent = SoftmaxScheduleAgent(bandit, alpha=0.1, t=1, tau_decay=0.01)\n",
        "    agent.train(200)\n",
        "\n",
        "    rewards_epsilon_softmax.append(agent.Q_t)\n",
        "    optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "  rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "  optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "  print(f\"{mean}: {rewards_epsilon_softmax}\")\n",
        "  print(f\"{mean}: {optimal_epsilon_softmax}\")\n",
        "  # result is decreasing fast the higher the mean gets\n",
        "  # result is relatively stable the higher the std gets --> (0,2) is actually good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JBFoYUAnFOH"
      },
      "source": [
        "## Optimistic initialization\n",
        "\n",
        "The initial estimates of 0 are now very **pessimistic** compared to the average reward you can get (between 10 and 20). This was not the case in the original setup.\n",
        "\n",
        "**Q:** Modify the classes so that they accept a parameter `Q_init` allowing to initialize the estimates `Q_t` to something different from 0. Change the initial value of the estimates to 10 for each algorithm. What happens? Conclude on the importance of reward scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EaWQf7uynFOI",
        "outputId": "532ade1e-b255-4829-f628-9d81b020f7c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10: [12.02142187 11.35868782 11.9686761  10.90397739 10.98431892]\n",
            "10: 0.83\n"
          ]
        }
      ],
      "source": [
        "nb_actions = 5\n",
        "\n",
        "rewards_epsilon_softmax = []\n",
        "optimal_epsilon_softmax = []\n",
        "\n",
        "for i in range(200):\n",
        "  bandit = Bandit(nb_actions, mean=10, std_Q=10, std_r=10)\n",
        "  agent = SoftmaxAgent(bandit, alpha=0.1, t=1, q_init=10)\n",
        "  agent.train(200)\n",
        "\n",
        "  rewards_epsilon_softmax.append(agent.Q_t)\n",
        "  optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "print(f\"{10}: {rewards_epsilon_softmax}\")\n",
        "print(f\"{10}: {optimal_epsilon_softmax}\")\n",
        "\n",
        "# result is depending on the Q-init value --> closer to the real value --> better result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3owmxt-knFOI"
      },
      "source": [
        "Let's now use **optimistic initialization**, i.e. initialize the estimates to a much higher value than what is realistic.\n",
        "\n",
        "**Q:** Implement optimistic initialization by initializing the estimates of all three algorithms to 25. What happens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hOg7DMPRnFOJ",
        "outputId": "5f707ee5-482b-4caf-a9ff-1357a23eef1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Agent\n",
            "10: [16.74790221 17.30600229 17.37566872 17.23710656 16.94093117]\n",
            "10: 0.835\n",
            "Epsilon Greedy Agent\n",
            "10: [16.03897558 16.414537   16.44114822 15.96470697 16.44338848]\n",
            "10: 0.825\n",
            "Greedy Agent\n",
            "10: [16.80129973 17.13688241 17.13887764 17.21683183 16.89081345]\n",
            "10: 0.84\n"
          ]
        }
      ],
      "source": [
        "nb_actions = 5\n",
        "q_init = 50\n",
        "\n",
        "print(\"Softmax Agent\")\n",
        "\n",
        "rewards_epsilon_softmax = []\n",
        "optimal_epsilon_softmax = []\n",
        "\n",
        "for i in range(200):\n",
        "  bandit = Bandit(nb_actions, mean=10, std_Q=10, std_r=10)\n",
        "  agent = SoftmaxAgent(bandit, alpha=0.1, t=1, q_init=q_init)\n",
        "  agent.train(200)\n",
        "\n",
        "  rewards_epsilon_softmax.append(agent.Q_t)\n",
        "  optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "print(f\"{10}: {rewards_epsilon_softmax}\")\n",
        "print(f\"{10}: {optimal_epsilon_softmax}\")\n",
        "\n",
        "print(\"Epsilon Greedy Agent\")\n",
        "\n",
        "rewards_epsilon_softmax = []\n",
        "optimal_epsilon_softmax = []\n",
        "\n",
        "for i in range(200):\n",
        "  bandit = Bandit(nb_actions, mean=10, std_Q=10, std_r=10)\n",
        "  agent = EpsilonGreedyAgent(bandit, alpha=0.1, epsilon=0.1, q_init=q_init)\n",
        "  agent.train(200)\n",
        "\n",
        "  rewards_epsilon_softmax.append(agent.Q_t)\n",
        "  optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "print(f\"{10}: {rewards_epsilon_softmax}\")\n",
        "print(f\"{10}: {optimal_epsilon_softmax}\")\n",
        "\n",
        "print(\"Greedy Agent\")\n",
        "\n",
        "rewards_epsilon_softmax = []\n",
        "optimal_epsilon_softmax = []\n",
        "\n",
        "for i in range(200):\n",
        "  bandit = Bandit(nb_actions, mean=10, std_Q=10, std_r=10)\n",
        "  agent = GreedyAgent(bandit, alpha=0.1, q_init=q_init)\n",
        "  agent.train(200)\n",
        "\n",
        "  rewards_epsilon_softmax.append(agent.Q_t)\n",
        "  optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "print(f\"{10}: {rewards_epsilon_softmax}\")\n",
        "print(f\"{10}: {optimal_epsilon_softmax}\")\n",
        "\n",
        "\n",
        "# somehow we saw a better result at a starting point that is too high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly9iBY3PnFOK"
      },
      "source": [
        "## Reinforcement comparison\n",
        "\n",
        "The problem with the previous **value-based** methods is that the Q-value estimates depend on the absolute magnitude of the rewards (by definition). The hyperparameters of the learning algorithms (learning rate, exploration, initial values) will therefore be very different depending on the scaling of the rewards (between 0 and 1, between -100 and 100, etc).\n",
        "\n",
        "A way to get rid of this dependency is to introduce **preferences** $p_t(a)$ for each action instead of the estimated Q-values. Preferences should follow the Q-values: an action with a high Q-value should have a high Q-value and vice versa, but we do not care about its exact scaling.\n",
        "\n",
        "In **reinforcement comparison**, we introduce a baseline $\\tilde{r}_t$ which is the average received reward **regardless the action**, i.e. there is a single value for the whole problem. This average reward is simply updated after each action with a moving average of the received rewards:\n",
        "\n",
        "$$\\tilde{r}_{t+1} = \\tilde{r}_{t} + \\alpha \\, (r_t - \\tilde{r}_{t})$$\n",
        "\n",
        "The average reward is used to update the preference for the action that was just executed:\n",
        "\n",
        "$$p_{t+1}(a_t) = p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_{t})$$\n",
        "\n",
        "If the action lead to more reward than usual, its preference should be increased (good surprise). If the action lead to less reward than usual, its preference should be decreased (bad surprise).\n",
        "\n",
        "Action selection is simply a softmax over the preferences, without the temperature parameter (as we do not care about the scaling):\n",
        "\n",
        "$$\n",
        "    \\pi (a) = \\frac{\\exp p_t(a)}{ \\sum_b \\exp p_t(b)}\n",
        "$$\n",
        "\n",
        "**Q:** Implement reinforcement comparison (with $\\alpha=\\beta=0.1$) and compare it to the other methods on the default settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mTbEWG9wnFOK"
      },
      "outputs": [],
      "source": [
        "class ReinforcementComparisonAgent:\n",
        "    def __init__(self, bandit, alpha, beta, q_init):\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "        # create an array filled with q_init\n",
        "        self.Q_t = np.zeros(bandit.nb_actions)\n",
        "        self.Q_t.fill(q_init)\n",
        "\n",
        "        self.baseline = 0\n",
        "        self.preference = np.zeros(bandit.nb_actions)\n",
        "\n",
        "        self.optimal_action = []\n",
        "\n",
        "    def act(self):\n",
        "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.Q_t[action] = self.Q_t[action] + self.alpha * (reward - self.Q_t[action])\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "        for t in range(nb_steps):\n",
        "          action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
        "\n",
        "          # softmax selection\n",
        "          prob = []\n",
        "\n",
        "          for a in range(self.bandit.nb_actions):\n",
        "            sum = 0\n",
        "            for b in range(self.bandit.nb_actions):\n",
        "                sum += np.exp(self.preference[b])\n",
        "            print(sum)\n",
        "            print(np.exp(self.preference[a]))\n",
        "            prob.append(np.exp(self.preference[a])) / sum\n",
        "\n",
        "          # chose reward based on probabilites\n",
        "          action = rng.choice(range(self.bandit.nb_actions), p=prob)\n",
        "\n",
        "          reward = self.bandit.step(action)\n",
        "          self.update(action, reward)\n",
        "\n",
        "          # update the reinforcement comparison\n",
        "          self.baseline = self.baseline + self.alpha * (reward - self.baseline)\n",
        "          self.preference[action] = self.preference[action] + self.beta * (reward - self.baseline)\n",
        "\n",
        "          if a == self.bandit.a_star:\n",
        "            self.optimal_action.append(1)\n",
        "          else:\n",
        "            self.optimal_action.append(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRcGQ9aanFOL"
      },
      "source": [
        "**Q:** Compare all methods with optimistic initialization. The true Q-values come from $\\mathcal{N}(10, 10)$, the estimated Q-values are initialized to 20 for greedy, $\\epsilon$-greedy and softmax, and the average reward is initialized to 20 for RC (the preferences are initialized at 0).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oen2kTrOnFOL",
        "outputId": "214f070f-3737-4d2c-c559-58b94412109d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Agent\n",
            "5.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-28dd56ef925e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_Q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReinforcementComparisonAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mrewards_epsilon_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-9c9db863fab2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_steps)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m# chose reward based on probabilites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
          ]
        }
      ],
      "source": [
        "nb_actions = 5\n",
        "q_init = 50\n",
        "\n",
        "print(\"Softmax Agent\")\n",
        "\n",
        "rewards_epsilon_softmax = []\n",
        "optimal_epsilon_softmax = []\n",
        "\n",
        "for i in range(200):\n",
        "  bandit = Bandit(nb_actions, mean=10, std_Q=10, std_r=10)\n",
        "  agent = ReinforcementComparisonAgent(bandit, alpha=0.1, beta=0.1, q_init=q_init)\n",
        "  agent.train(200)\n",
        "\n",
        "  rewards_epsilon_softmax.append(agent.Q_t)\n",
        "  optimal_epsilon_softmax.append(1) if agent.act() == bandit.a_star else optimal_epsilon_softmax.append(0)\n",
        "\n",
        "rewards_epsilon_softmax = np.mean(rewards_epsilon_softmax, axis=0)\n",
        "optimal_epsilon_softmax = np.mean(optimal_epsilon_softmax, axis=0)\n",
        "\n",
        "print(f\"{10}: {rewards_epsilon_softmax}\")\n",
        "print(f\"{10}: {optimal_epsilon_softmax}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ymxj2CKx4ZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d24234067c217f49dc985cbc60012ce72928059d528f330ba9cb23ce737906d"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}